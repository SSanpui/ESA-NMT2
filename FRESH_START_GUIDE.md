# Complete Fresh Setup Guide for Colab (After Session Loss)

## What Happened

**"Restart session"** in Colab disconnects your session and you lose all files in `/content/`. This is different from "Restart runtime" which only clears memory.

**Lost files:**
- ‚ùå BHT25_All_annotated.csv (3 hours of work)
- ‚ùå Demo output files
- ‚ùå All code files

## Fresh Start - Complete Setup

### Cell 1: Clone Repository (With All CUDA Fixes)

```python
import os
from google.colab import drive

# Mount Google Drive to save important files
drive.mount('/content/drive')

# Create project directory in Drive (to prevent data loss)
project_dir = '/content/drive/MyDrive/ESA_NMT_Project'
os.makedirs(project_dir, exist_ok=True)

print(f"‚úÖ Drive mounted. Project dir: {project_dir}")

# Clone the repository with all fixes
if os.path.exists('ESA-NMT'):
    print("‚ö†Ô∏è  ESA-NMT already exists, removing...")
    !rm -rf ESA-NMT

print("üì• Cloning repository with CUDA fixes...")
!git clone https://github.com/SSanpui/ESA-NMT.git
%cd ESA-NMT

# Checkout the correct branch with all fixes
!git checkout claude/indictrans2-emotion-translation-011CULAwXFzu13RU7C1NhByj

print("\n‚úÖ Repository cloned!")
print("Branch:", !git branch --show-current)

# List CUDA fix files
print("\nüìÅ CUDA Fix Files:")
!ls -lh fix_cuda_error.py diagnose_cuda_error.py colab_cell_full_training_safe.py 2>/dev/null || echo "Files should be present"
```

### Cell 2: Install Dependencies

```python
print("üì¶ Installing dependencies...")

!pip install -q torch transformers sentencepiece sacrebleu rouge-score bert-score sentence-transformers
!pip install -q accelerate sacremoses datasets tokenizers protobuf
!pip install -q pandas numpy matplotlib seaborn tqdm

print("‚úÖ All dependencies installed!")
```

### Cell 3: Verify Dataset

```python
import pandas as pd
import os

# Check if original dataset exists
if os.path.exists('BHT25_All.csv'):
    df = pd.read_csv('BHT25_All.csv')
    print(f"‚úÖ Dataset found: {len(df)} rows")
    print(f"Columns: {df.columns.tolist()}")
    print(f"Size: {os.path.getsize('BHT25_All.csv') / 1024**2:.1f} MB")
else:
    print("‚ùå BHT25_All.csv not found!")
    print("You need to upload it first.")
```

### Cell 4: Smart Annotation Check

```python
import os

# Check if annotated file already exists in Drive (backup location)
drive_annotated = '/content/drive/MyDrive/ESA_NMT_Project/BHT25_All_annotated.csv'
local_annotated = 'BHT25_All_annotated.csv'

if os.path.exists(drive_annotated):
    print("üéâ Found annotated CSV in Google Drive backup!")
    print("Copying from Drive to local...")
    !cp "$drive_annotated" "$local_annotated"

    # Verify
    import pandas as pd
    df = pd.read_csv(local_annotated)
    print(f"‚úÖ Restored: {len(df)} rows")
    print(f"Columns: {df.columns.tolist()}")

    # Check emotion range
    if 'emotion_bn' in df.columns:
        emotion_values = df['emotion_bn'].values
        print(f"Emotion range: [{emotion_values.min()}, {emotion_values.max()}]")

        if emotion_values.max() <= 3:
            print("‚úÖ Dataset is valid (4 emotions)!")
            print("\nüëâ Skip to Cell 6 (Apply CUDA Fix)")
        else:
            print("‚ö†Ô∏è  Dataset has 8 emotions, need to re-annotate")

elif os.path.exists(local_annotated):
    print("‚úÖ Annotated CSV found locally!")
    print("Backing up to Drive...")
    !cp "$local_annotated" "$drive_annotated"
    print("‚úÖ Backed up to Drive!")
    print("\nüëâ Skip to Cell 6 (Apply CUDA Fix)")

else:
    print("‚ùå No annotated CSV found.")
    print("üëâ Continue to Cell 5 (Run Annotation)")
```

### Cell 5: Run Annotation (ONLY if needed - takes ~3 hours)

```python
import os
from google.colab import drive

print("‚è∞ Starting annotation... This will take ~3 hours!")
print("üìä Progress will be shown below.")

# Run annotation
!python annotate_dataset.py

# Verify output
if os.path.exists('BHT25_All_annotated.csv'):
    import pandas as pd
    df = pd.read_csv('BHT25_All_annotated.csv')
    print(f"\n‚úÖ Annotation completed: {len(df)} rows")

    # IMMEDIATELY backup to Google Drive
    drive_backup = '/content/drive/MyDrive/ESA_NMT_Project/BHT25_All_annotated.csv'
    !cp BHT25_All_annotated.csv "$drive_backup"
    print(f"‚úÖ BACKED UP to Drive: {drive_backup}")
    print("   Your 3-hour work is now safe in Google Drive!")

    # Also show statistics
    emotion_values = df['emotion_bn'].values
    emotion_names = ['joy', 'sadness', 'anger', 'fear']
    print("\nüìä Emotion Distribution:")
    for i in range(4):
        count = (emotion_values == i).sum()
        pct = count / len(emotion_values) * 100
        print(f"   {i} ({emotion_names[i]:8s}): {count:5d} ({pct:5.1f}%)")
else:
    print("‚ùå Annotation failed!")
```

### Cell 6: Apply CUDA Error Fix

```python
print("üîß Applying CUDA error fix...")

# Run the fix script
!python fix_cuda_error.py

print("\n‚úÖ CUDA fix applied!")
print("\n‚ö†Ô∏è  Important: DO NOT restart session!")
print("    Continue to next cell directly.")
```

### Cell 7: Configuration

```python
# Set your configuration
TRANSLATION_PAIR = "bn-hi"  # or "bn-te"
MODEL_TYPE = "nllb"
NUM_EPOCHS = 3
BATCH_SIZE = 2

print(f"üìã Configuration:")
print(f"   Translation: {TRANSLATION_PAIR}")
print(f"   Model: {MODEL_TYPE}")
print(f"   Epochs: {NUM_EPOCHS}")
print(f"   Batch Size: {BATCH_SIZE}")
```

### Cell 8: Run Training (Safe Version)

```python
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Better error messages

from emotion_semantic_nmt_enhanced import full_training_pipeline

print("üöÄ Starting Full Training...")
print("="*70)

try:
    metrics = full_training_pipeline(
        csv_path='BHT25_All.csv',  # Will auto-load annotated version
        translation_pair=TRANSLATION_PAIR,
        model_type=MODEL_TYPE
    )

    print("\n" + "="*70)
    print("‚úÖ TRAINING COMPLETED!")
    print("="*70)

    # Save results to Drive
    import json
    from emotion_semantic_nmt_enhanced import ComprehensiveEvaluator

    results_path = f'/content/drive/MyDrive/ESA_NMT_Project/training_results_{MODEL_TYPE}_{TRANSLATION_PAIR}.json'
    with open(results_path, 'w') as f:
        json.dump(ComprehensiveEvaluator.convert_to_json_serializable(metrics), f, indent=2)

    print(f"\nüíæ Results saved to Drive: {results_path}")

    # Copy checkpoint to Drive
    checkpoint_local = f'./checkpoints/final_model_{MODEL_TYPE}_{TRANSLATION_PAIR}.pt'
    checkpoint_drive = f'/content/drive/MyDrive/ESA_NMT_Project/final_model_{MODEL_TYPE}_{TRANSLATION_PAIR}.pt'

    if os.path.exists(checkpoint_local):
        !cp "$checkpoint_local" "$checkpoint_drive"
        print(f"üíæ Model saved to Drive: {checkpoint_drive}")

    print("\nüéâ All outputs backed up to Google Drive!")

except Exception as e:
    print("\n‚ùå Error during training:")
    print(e)
    print("\nRun: !python diagnose_cuda_error.py")

```

### Cell 9: (Optional) Save Work to Drive Manually

```python
# Run this periodically to backup everything

import shutil
from datetime import datetime

backup_dir = f'/content/drive/MyDrive/ESA_NMT_Project/backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
os.makedirs(backup_dir, exist_ok=True)

# Backup important files
files_to_backup = [
    'BHT25_All_annotated.csv',
    './outputs/',
    './checkpoints/'
]

for item in files_to_backup:
    if os.path.exists(item):
        if os.path.isdir(item):
            shutil.copytree(item, f"{backup_dir}/{os.path.basename(item)}", dirs_exist_ok=True)
        else:
            shutil.copy(item, backup_dir)
        print(f"‚úÖ Backed up: {item}")

print(f"\nüíæ Backup completed: {backup_dir}")
```

## Key Changes to Prevent Data Loss

1. **Mount Google Drive** - Store important files there
2. **Auto-backup annotated CSV** - Immediately after annotation
3. **Check Drive for existing files** - Don't re-annotate if backup exists
4. **Backup checkpoints** - Save trained models to Drive
5. **Save results to Drive** - Keep all outputs safe

## Summary

**Run cells in order:**
1. ‚úÖ Clone repo (Cell 1)
2. ‚úÖ Install packages (Cell 2)
3. ‚úÖ Check dataset (Cell 3)
4. ‚úÖ Check for backup in Drive (Cell 4)
5. ‚è∞ Annotate ONLY if no backup found (Cell 5) - 3 hours
6. üîß Apply CUDA fix (Cell 6)
7. ‚öôÔ∏è Configure (Cell 7)
8. üöÄ Train (Cell 8)

**Important:** Cell 4 will check if you have a backup in Google Drive, so you might not need to wait 3 hours again!

## If You Have the Annotated CSV Elsewhere

If you downloaded `BHT25_All_annotated.csv` to your computer before:

```python
from google.colab import files
uploaded = files.upload()  # Upload BHT25_All_annotated.csv

# Then backup to Drive
!cp BHT25_All_annotated.csv /content/drive/MyDrive/ESA_NMT_Project/
print("‚úÖ Uploaded and backed up!")
```

Then skip to Cell 6!
