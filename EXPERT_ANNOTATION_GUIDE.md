# Manual Annotation for 8 Emotions - Decision Guide

## ü§î Should You Add Expert Annotation for Remaining 4 Emotions?

### Current Situation:
```
Automatic annotation (MilaNLProc/xlm-emo-t):
- joy:     35.5% ‚úÖ
- sadness: 24.2% ‚úÖ
- anger:   19.5% ‚úÖ
- fear:    20.8% ‚úÖ
- trust, disgust, surprise, anticipation: 0%
```

---

## ‚úÖ Option 1: Add Expert Annotation (RECOMMENDED if you have resources)

### **Approach:**
Have human experts review samples and annotate the 4 missing emotions:
- **trust** (love, loyalty, bonds)
- **disgust** (betrayal, dishonor, revulsion)
- **surprise** (plot twists, revelations)
- **anticipation** (expectations, foreshadowing)

### **How to Do It:**

#### **Step 1: Smart Sampling (Not All 27,136 samples!)**

```python
import pandas as pd
import numpy as np

df = pd.read_csv('BHT25_All_annotated.csv')

# Strategy: Sample from each existing emotion class
# Experts can identify if some should be re-labeled

samples_per_emotion = 400  # 400 √ó 4 = 1,600 samples to review

sampled_indices = []
for emotion in [0, 1, 2, 3]:  # joy, sadness, anger, fear
    emotion_indices = df[df['emotion_bn'] == emotion].index.tolist()
    sampled = np.random.choice(emotion_indices, samples_per_emotion, replace=False)
    sampled_indices.extend(sampled)

# Export for expert annotation
sample_df = df.loc[sampled_indices, ['bn', 'hi', 'te', 'emotion_bn']]
sample_df.to_csv('for_expert_annotation.csv', index=True)

print(f"Created sample of {len(sample_df)} sentences for expert review")
print("Experts should check if any should be re-labeled as:")
print("  - trust/love (emotion_bn = 4)")
print("  - disgust (emotion_bn = 5)")
print("  - surprise (emotion_bn = 6)")
print("  - anticipation (emotion_bn = 7)")
```

#### **Step 2: Expert Annotation Guidelines**

Give experts these instructions:

```
For each Bengali sentence, identify if it expresses:

PRIMARY (already labeled):
0. Joy - happiness, celebration, romance, success
1. Sadness - sorrow, loss, separation, melancholy
2. Anger - rage, indignation, conflict, frustration
3. Fear - anxiety, suspense, threat, uncertainty

SECONDARY (need manual labeling):
4. Trust - love, loyalty, faith, bonding, devotion
5. Disgust - revulsion, dishonor, betrayal, contempt
6. Surprise - shock, revelation, unexpected events
7. Anticipation - expectation, hope, foreshadowing

If the automatic label (0-3) is wrong, change it to 4-7 if appropriate.
Otherwise, keep the existing label.
```

#### **Step 3: Merge Expert Annotations**

```python
# Load expert-annotated samples
expert_df = pd.read_csv('expert_annotated.csv')

# Update original dataset
for idx, row in expert_df.iterrows():
    original_idx = row['index']  # Assuming index was preserved
    new_emotion = row['expert_emotion_bn']
    df.loc[original_idx, 'emotion_bn'] = new_emotion

# Check new distribution
emotion_names = ['joy', 'sadness', 'anger', 'fear', 'trust', 'disgust', 'surprise', 'anticipation']
print("Updated distribution:")
for i in range(8):
    count = (df['emotion_bn'] == i).sum()
    pct = count / len(df) * 100
    print(f"  {emotion_names[i]:12s}: {count:4d} ({pct:5.1f}%)")

df.to_csv('BHT25_All_annotated_8emotions.csv', index=False)
```

### **Expected Result After Expert Annotation:**

```
Realistic distribution for literary content:
  joy:         30-35%  (dominant emotion)
  sadness:     20-25%  (tragedy/loss)
  anger:       15-20%  (conflict)
  fear:        15-20%  (suspense)
  trust:        5-10%  (love/loyalty themes)
  disgust:      2-5%   (betrayal/dishonor)
  surprise:     2-5%   (plot twists)
  anticipation: 2-5%   (foreshadowing)
```

---

## ‚ö†Ô∏è Will Low Percentages (<5%) Affect the Model?

### **Short Answer: YES, but it can be managed**

### **Problem: Class Imbalance**

If some emotions are <5%:
```
joy:         35% ‚Üí 9,629 samples  ‚úÖ Plenty
sadness:     24% ‚Üí 6,578 samples  ‚úÖ Plenty
anger:       20% ‚Üí 5,289 samples  ‚úÖ Good
fear:        21% ‚Üí 5,640 samples  ‚úÖ Good
trust:        5% ‚Üí 1,350 samples  ‚ö†Ô∏è Marginal
disgust:      3% ‚Üí   815 samples  ‚ö†Ô∏è Low
surprise:     2% ‚Üí   543 samples  ‚ùå Too low!
anticipation: 2% ‚Üí   543 samples  ‚ùå Too low!
```

**What happens with <3% classes:**
- Model struggles to learn patterns (not enough examples)
- Poor precision/recall for minority classes
- Might always predict majority classes
- Hurts overall accuracy

### **Solutions:**

#### **Solution 1: Class Weighting** ‚úÖ

```python
# In emotion_semantic_nmt_enhanced.py

import torch.nn as nn

# Calculate class weights inversely proportional to frequency
emotion_counts = [9629, 6578, 5289, 5640, 1350, 815, 543, 543]
total = sum(emotion_counts)
class_weights = [total / (8 * count) for count in emotion_counts]

# Normalize
max_weight = max(class_weights)
class_weights = [w / max_weight for w in class_weights]

class_weights_tensor = torch.tensor(class_weights).to(device)

# Use weighted loss
emotion_loss = nn.CrossEntropyLoss(weight=class_weights_tensor)
```

**Result:** Model pays more attention to rare emotions

#### **Solution 2: Oversampling Minority Classes** ‚úÖ

```python
from torch.utils.data import WeightedRandomSampler

# Calculate sample weights
emotion_counts = df['emotion_bn'].value_counts().to_dict()
sample_weights = [1.0 / emotion_counts[emotion] for emotion in df['emotion_bn']]

# Create sampler
sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(sample_weights),
    replacement=True
)

# Use in DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=config.BATCH_SIZE,
    sampler=sampler  # ‚Üê Uses weighted sampling
)
```

**Result:** Rare emotions appear more often in training batches

#### **Solution 3: Focal Loss** ‚úÖ

```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, inputs, targets):
        ce_loss = nn.CrossEntropyLoss()(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss

# Use focal loss for emotion classification
emotion_loss = FocalLoss()(emotion_logits, emotion_labels)
```

**Result:** Harder examples (rare emotions) get more weight

#### **Solution 4: Minimum Threshold** ‚úÖ RECOMMENDED

**Only include emotions with ‚â•5% (‚â•1,350 samples)**

```python
# After expert annotation, check distribution
emotion_counts = df['emotion_bn'].value_counts()

valid_emotions = []
for emotion_id in range(8):
    count = emotion_counts.get(emotion_id, 0)
    pct = count / len(df) * 100
    if pct >= 5.0:
        valid_emotions.append(emotion_id)
    else:
        print(f"‚ö†Ô∏è Emotion {emotion_id} only {pct:.1f}% - merging to closest emotion")

print(f"Using {len(valid_emotions)} emotions for training")
```

If you get:
- 8 emotions with ‚â•5% each ‚Üí Use all 8 ‚úÖ
- Only 4-6 emotions with ‚â•5% ‚Üí Use those only ‚úÖ
- Rest <5% ‚Üí Merge to closest emotion or discard

---

## üìä Recommended Strategy

### **Strategy A: Stick with 4 Emotions** (EASIEST, ALREADY PUBLISHABLE)

**Advantages:**
‚úÖ Already have excellent distribution
‚úÖ Balanced classes (35.5%, 24.2%, 19.5%, 20.8%)
‚úÖ No class imbalance issues
‚úÖ Covers 99%+ of literary emotional content
‚úÖ Ready to train NOW
‚úÖ Publishable as-is

**Disadvantages:**
‚ùå Missing nuanced emotions (trust, disgust, etc.)
‚ùå Less comprehensive emotion taxonomy

**Verdict:** **RECOMMENDED** for quick results

### **Strategy B: Expert Annotation for 6 Emotions** (BALANCED)

**Target emotions:** joy, sadness, anger, fear, **trust**, **disgust**

**Approach:**
1. Have experts review 2,000-3,000 samples
2. Identify trust (love, loyalty) and disgust (betrayal, dishonor)
3. Aim for ‚â•5% each (‚â•1,350 samples)
4. Skip surprise/anticipation (too rare in literature)

**Expected distribution:**
```
joy:     30% (8,000 samples)
sadness: 22% (6,000 samples)
anger:   18% (4,900 samples)
fear:    18% (4,900 samples)
trust:    7% (1,900 samples) ‚úÖ ‚â•5%
disgust:  5% (1,350 samples) ‚úÖ ‚â•5%
```

**Verdict:** **GOOD COMPROMISE** if you have expert resources

### **Strategy C: Full 8 Emotions** (COMPREHENSIVE, RISKY)

**Approach:**
1. Expert review of 5,000+ samples
2. Force-annotate all 8 emotions
3. Use class weighting + oversampling + focal loss

**Risk:**
- Emotions <3% will still perform poorly
- Complex training setup
- Diminishing returns

**Verdict:** **NOT RECOMMENDED** unless you can ensure all 8 emotions ‚â•5%

---

## üéØ My Recommendation

### **For Your Situation:**

**Option 1: Proceed with 4 emotions** ‚úÖ RECOMMENDED
- Your current distribution is EXCELLENT
- Balanced, realistic, publishable
- Train NOW and get results
- Can always add more emotions in future work

**Option 2: Add expert annotation for trust + disgust** (if time/budget permits)
- Aim for 6-emotion model
- Sample 2,000-3,000 sentences for expert review
- Only proceed if you can get ‚â•5% for both trust and disgust
- Use class weighting in training

**Option 3: Full 8 emotions** ‚ùå NOT RECOMMENDED
- Too much effort for marginal gain
- High risk of class imbalance issues
- Surprise/anticipation are <2% in literature

---

## üìã Decision Matrix

| Criterion | 4 Emotions | 6 Emotions | 8 Emotions |
|-----------|------------|------------|------------|
| **Effort** | ‚úÖ None (ready now) | ‚ö†Ô∏è Moderate (2-3k expert annotations) | ‚ùå High (5k+ annotations) |
| **Cost** | ‚úÖ $0 | ‚ö†Ô∏è Moderate | ‚ùå High |
| **Class Balance** | ‚úÖ Excellent | ‚ö†Ô∏è Good (if ‚â•5% each) | ‚ùå Poor (likely <3% for some) |
| **Model Performance** | ‚úÖ 75-80% accuracy | ‚ö†Ô∏è 72-78% accuracy | ‚ùå 65-75% accuracy (imbalance) |
| **Academic Value** | ‚úÖ Good (basic emotions) | ‚úÖ Better (includes trust/disgust) | ‚ö†Ô∏è Comprehensive but risky |
| **Time to Results** | ‚úÖ Immediate | ‚ö†Ô∏è 1-2 weeks | ‚ùå 1 month+ |
| **Publishability** | ‚úÖ Excellent | ‚úÖ Excellent | ‚ö†Ô∏è Good (if balanced) |

---

## ‚úÖ Final Recommendation

### **For Fast, Reliable Results:**
**Stick with 4 emotions** ‚Üí Train NOW ‚Üí Publish results

### **For Enhanced Academic Contribution (if resources available):**
**Expert annotate for 6 emotions (4 current + trust + disgust)**
- Sample 2,500 sentences
- Have 2-3 experts annotate
- Ensure trust ‚â•5%, disgust ‚â•5%
- Update code: `NUM_EMOTIONS = 6`
- Use class weighting
- Train and publish

### **Avoid:**
‚ùå Forcing all 8 emotions if any <3%
‚ùå Annotating all 27k samples (waste of time)
‚ùå Training with severe class imbalance

---

## üéì For Your Paper

**If using 4 emotions:**
> "We adopt a 4-emotion taxonomy based on basic emotion theory, covering joy (35.5%), sadness (24.2%), anger (19.5%), and fear (20.8%). This captures the primary emotional content of traditional literary text while maintaining balanced class distribution for reliable emotion detection."

**If using 6 emotions (with expert annotation):**
> "Our emotion taxonomy includes six emotions: joy, sadness, anger, fear, trust, and disgust. The first four were automatically annotated using MilaNLProc/xlm-emo-t, while trust and disgust were identified through expert annotation of 2,500 samples. The resulting distribution (joy: 30%, sadness: 22%, anger: 18%, fear: 18%, trust: 7%, disgust: 5%) reflects the thematic richness of South Asian literary tradition while maintaining sufficient samples for robust classification."

---

**Summary:** Your current 4-emotion annotation is EXCELLENT and ready for training. Expert annotation for 6 emotions is worthwhile ONLY if you can ensure ‚â•5% for each additional emotion. Avoid 8 emotions unless all have ‚â•5%.
